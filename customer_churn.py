# -*- coding: utf-8 -*-
"""Customer_Churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rSJe7pSA6p0FMeAE5xW8Qi1Ww7H8rBBd
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Load dataset
# The original path was a Windows path and the notebook is running in a Linux environment.
# Update the path to the location where the CSV file is located in the Linux environment.
# For example, if the file is in the same directory as the notebook:
try:
    df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
except FileNotFoundError:
    print("Error: The file 'WA_Fn-UseC_-Telco-Customer-Churn.csv' was not found.")
    print("Please ensure the file is in the correct directory or provide the full path in the Linux environment.")
    # As a diagnostic step, you could try listing the files in the current directory:
    # import os
    # print("Files in the current directory:", os.listdir('.'))

df.head()

# Drop customerID
df.drop('customerID', axis=1, inplace=True)

# Convert TotalCharges to numeric
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

# Convert target to 0/1
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

# Encode categorical variables
categorical_cols = df.select_dtypes(include=['object']).columns
le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# Churn count
sns.countplot(x='Churn', data=df)
plt.title('Churn Count')
plt.show()

# Heatmap of correlation
plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Feature Correlation')
plt.show()

X = df.drop('Churn', axis=1)
y = df['Churn']

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Classifier
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

!pip install streamlit

# Save this in a file named 'app.py'

# Install Streamlit if it's not already installed
!pip install streamlit

import streamlit as st
import pandas as pd
import pickle

# Ensure the model file exists and the path is correct
# This part assumes you have saved your trained model to 'churn_model.pkl'
# You would typically do this after training the model, e.g.:
# import pickle
# pickle.dump(model, open('churn_model.pkl', 'wb'))
try:
    model = pickle.load(open('churn_model.pkl', 'rb'))
except FileNotFoundError:
    st.error("Error: 'churn_model.pkl' not found. Please ensure the trained model is saved to this file.")
    st.stop() # Stop the Streamlit app if the model is not found


st.title("Customer Churn Predictor")

gender = st.selectbox("Gender", ['Male', 'Female'])
senior = st.selectbox("Senior Citizen", [0, 1])
tenure = st.slider("Tenure (Months)", 0, 72)
monthly = st.slider("Monthly Charges", 0, 150)
total = st.slider("Total Charges", 0, 9000)

# Dummy example â€” you'd build features properly here
# The columns in input_data should match the features your model was trained on
# and should be in the same order. You'll need to apply the same preprocessing
# (like label encoding) here as you did during training.
# This dummy example only includes a few features.
# You need to add all features required by your trained model and handle their types correctly.
# For instance, 'gender' would need to be encoded to a numerical value (0 or 1)
# just like you did during training using LabelEncoder.
# A better approach would be to load the LabelEncoder objects used during training
# and apply them to the input data.
input_data = pd.DataFrame([[gender, senior, tenure, monthly, total]],
                          columns=['gender', 'SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges'])

# **Important:** Apply the same encoding/preprocessing to input_data here
# as was applied to the training data (X_train).
# For example, if you label encoded 'gender', you need to encode it here too.
# Since the original notebook applied LabelEncoder to all object columns,
# you would need to recreate that logic here or save the encoders.
# For simplicity in this example, let's assume you only need these 5 features
# and that 'gender' and 'SeniorCitizen' are handled appropriately (e.g., 'Female' is 0, 'Male' is 1).
# This dummy example might not work directly with your trained model if it expects
# more features or different encodings.

# Example of how you might handle encoding for 'gender' if it was encoded to 0/1:
input_data['gender'] = input_data['gender'].map({'Female': 0, 'Male': 1})


if st.button("Predict"):
    # Ensure input_data has the same columns and order as X_train
    # You might need to add other features with default or calculated values
    # if your model was trained on more features than collected by the sliders.
    # For instance, if 'Partner', 'Dependents', etc. were in your training data,
    # you need to include them in input_data, perhaps with default values or
    # by adding more input widgets in the Streamlit app.

    try:
        result = model.predict(input_data)
        st.success(f"The customer is likely to {'Churn' if result[0] == 1 else 'Stay'}")
    except Exception as e:
        st.error(f"An error occurred during prediction: {e}")
        st.info("Please check if the input features and their types match the model's expectations.")